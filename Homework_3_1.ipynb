{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import time\n",
    "import urllib.request\n",
    "import urllib.error as uer\n",
    "\n",
    "import lxml\n",
    "\n",
    "\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [06:12<00:00,  1.07it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 682872.28it/s]\n"
     ]
    }
   ],
   "source": [
    "anime = []\n",
    "for page in tqdm(range(0,400)):\n",
    "    url = \"https://myanimelist.net/topanime.php?limit \" +str(page *50)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    for tag in soup.find_all(\"tr\"):#cerco le righe della tabella\n",
    "        links = tag.find_all(\"a\")\n",
    "        for link in links:\n",
    "            if type(link.get(\"id\")) == str and len(link.contents[0]) >1:\n",
    "                anime.append((link.get(\"href\")))\n",
    "with open('urls.txt', 'w') as f:\n",
    "    for item in tqdm(anime):\n",
    "        f.write(\"%s\\n\" % item)      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading the html pages\n",
    "def html_downloading(l):\n",
    "    n = []\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            file = '/Users/michele/ADM2021/article_ '+ str(i) + '.html'   \n",
    "            urllib.request.urlretrieve(l[i], file)\n",
    "        #if the link does not exist, we pass to the next link \n",
    "        except uer.HTTPError:\n",
    "            pass\n",
    "        except Exception as error:\n",
    "            #if an exception occured, we append the number of the link to this list\n",
    "            n.append(i)\n",
    "            time.sleep(1200) #60s * 20m = 1200s\n",
    "        time.sleep(2)\n",
    "    \n",
    "    #it will run only if some of the previous html was not downloaded because an exception occured \n",
    "    if n != []:\n",
    "        for i in n:\n",
    "            try:\n",
    "                file = '/Users/michele/ADM2021/article_ '+ str(i) + '.html'\n",
    "                urllib.request.urlretrieve(l[i], file)\n",
    "            #if the link does not exist, we pass to the next link  \n",
    "            except uer.HTTPError:\n",
    "                pass\n",
    "            except Exception as error:\n",
    "                time.sleep(1200) #60s * 20m = 1200s\n",
    "            time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_downloading(anime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_urls(n_pages, url_file):\n",
    "    pathAncestor = os.path.join(\"./\", \"htmlpages\") # Path\n",
    "    os.mkdir(pathAncestor) # create the folder in the path\n",
    "    \n",
    "    for i in range(1,n_pages+1):\n",
    "        os.makedirs(os.path.join(pathAncestor, 'page ' + str(i))) # create sequentially the folders interested\n",
    "    \n",
    "    UrlsFiles = open(url_file, \"r\") # open the file in \"read (r)\" mode\n",
    "\n",
    "    headpart = \"https://myanimelist.net\"\n",
    "    counter_pages = 0\n",
    "    counter_html = 0\n",
    "    for x in UrlsFiles: # crawl each Urls associated to the book to be sure to download the corresponding html article\n",
    "        if counter_html % 100 == 0: # check every hundred html page to change the folder where we insert the html article\n",
    "            counter_pages = counter_pages + 1 \n",
    "\n",
    "        counter_html = counter_html + 1\n",
    "\n",
    "        subdirectory = pathAncestor + \"/page \" + str(counter_pages) # select the corresponding folder to insert the html article\n",
    "        article_name = \"/article_\"+str(counter_html)+\".html\" # set the number of i-th book\n",
    "\n",
    "        complete_path = subdirectory + article_name # insert the new complete path where create the html file\n",
    "        with open(complete_path, \"wb\") as ip_file:\n",
    "            link = headpart + x\n",
    "            import time\n",
    "\n",
    "            page = requests.get(link)\n",
    "            try:\n",
    "                page = requests.get(link)\n",
    "                break\n",
    "            except:\n",
    "                print(\"Connection refused by the server..\")\n",
    "\n",
    "                time.sleep(5)\n",
    "                print(\"Was a nice sleep, now let me continue...\")\n",
    "                continue\n",
    "             # request the page\n",
    "\n",
    "\n",
    "            soup = BeautifulSoup(page.text, features='lxml')\n",
    "\n",
    "            ip_file.write(soup.encode('utf-8'))\n",
    "            ip_file.close()\n",
    "\n",
    "    UrlsFiles.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in urls.text:\n",
    "    request.get(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
