{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OAmHLj0XDsQ"
   },
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dE37ft_VXDsT"
   },
   "source": [
    "Letizia Russo, Daniel Losada Molina and Michele Luca Puzzo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiJCtCwWXDsa",
    "outputId": "d084f566-4d1e-4870-c12d-2860dc65ecc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/michele/opt/anaconda3/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: joblib in /Users/michele/opt/anaconda3/lib/python3.8/site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: click in /Users/michele/opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /Users/michele/opt/anaconda3/lib/python3.8/site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: tqdm in /Users/michele/opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.47.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "w5D60sG_XDsU"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml\n",
    "import shutil\n",
    "#import functions as fun\n",
    "import urllib.request\n",
    "import urllib.error as uer\n",
    "import time\n",
    "import nltk\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jH1PMXgOZLmZ"
   },
   "outputs": [],
   "source": [
    "nFolders = 400\n",
    "\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun','Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "\n",
    "def get_urls(init_url, number_pages):\n",
    "    anime = []\n",
    "    for page in tqdm(range(0,number_pages)):\n",
    "        url = init_url +str(page *50)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        for tag in soup.find_all(\"tr\"):#cerco le righe della tabella\n",
    "            links = tag.find_all(\"a\")\n",
    "            for link in links:\n",
    "                if type(link.get(\"id\")) == str and len(link.contents[0]) >1 and link not in anime:\n",
    "                    anime.append((link.get(\"href\")))\n",
    "    anime = list(set(anime))\n",
    "    with open('urls.txt', 'w') as f:\n",
    "        for item in tqdm(anime):\n",
    "            f.write(\"%s\\n\" % item) \n",
    "\n",
    "            \n",
    "def crawl(url_file):\n",
    "    Path(\"directory\").mkdir(exist_ok=True)\n",
    "    for i in range(1,nFolders+1):\n",
    "        nomeCartella = 'cartella{}'.format(i)\n",
    "        Path(nomeCartella).mkdir(exist_ok=True)\n",
    "    num_lines = sum(1 for line in open(url_file))\n",
    "    pagesPerFolder = num_lines/nFolders\n",
    "    a = open(url_file, \"r\")\n",
    "    c = 0\n",
    "    lastFolder = 200\n",
    "    for i in a:\n",
    "        page = requests.get(i)\n",
    "        soup = BeautifulSoup(page.content, features =\"lxml\")\n",
    "        number = int(c/(pagesPerFolder)) + 1\n",
    "        if number != lastFolder:\n",
    "            print(\"I'm waiting\")\n",
    "            lastFolder = number\n",
    "            time.sleep(200)            \n",
    "        print(\"Going to save in cartella\" + str(number) + \", the page \" + str(c+1))\n",
    "        f = open(\"./Cartelle/cartella{}\".format(number) + \"/page_{}.html\".format(c+1), \"w\",encoding=\"utf-8\")\n",
    "        f.write(soup.prettify())\n",
    "        f.close()\n",
    "        c = c + 1\n",
    "        \n",
    "def findField(array, word):\n",
    "    for x in array:\n",
    "        #print(\"#####################################\")\n",
    "        #print(\" \".join((x.find(\"span\").text).split()))\n",
    "        #print(\"#####################################\")\n",
    "        if \" \".join((x.find(\"span\").text).split()) == word:\n",
    "            return x\n",
    "    return -1       \n",
    "        \n",
    "        \n",
    "def cleaner(text):\n",
    "    stop_words = set(stopwords.words('english')) # retrieve stop words\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\") # recognize the tokens\n",
    "    parole = tokenizer.tokenize(text) # tokenize the text\n",
    "    correct_words = [] # save the correct words to consider like tokens\n",
    "    for word in parole:\n",
    "        # check if the word is lower and it isn't a stop word or a number\n",
    "        if word.lower() not in stop_words and word.isalpha(): \n",
    "            word = PorterStemmer().stem(word) # use the stemmer function\n",
    "            correct_words.append(word.lower()) # insert the good token to lower case\n",
    "        \n",
    "    return correct_words\n",
    "\n",
    "def create_vocabulary(data):\n",
    "    ### Input == I use like input the dataset obtain in exercise 1 where i apply the clean text function\n",
    "    ### Output == I obtain a vocabulary, the keys are all tokens (with no repeat) contained in the synopsis for the each rows\n",
    "    ### for each token I define the index of the rows where the token is in the synopsis\n",
    "    voc = {}\n",
    "    for i, row in data.iterrows():\n",
    "            if len(data.at[i, \"Synopsis\"]) > 0:  # check if the list is empty or not to avoid the eventually error\n",
    "                for word in data.at[i, \"Synopsis\"]: # bring the token from the list\n",
    "                    if word in voc.keys(): # insert the token into the vocabulary with the documents where this is present\n",
    "                        if i not in voc[word]:\n",
    "                            voc[word].append(i)\n",
    "                    else:\n",
    "                        voc[word] = [i]\n",
    "    return voc\n",
    "\n",
    "def extractData(pagePath):\n",
    "    with open(pagePath, encoding=\"utf-8\") as fp:\n",
    "        soup = BeautifulSoup(fp, \"html.parser\")\n",
    "        animeTitle = \" \".join((soup.html.head.title.text).split())\n",
    "        \n",
    "        #print(\"animeTitle: \", animeTitle)\n",
    "        animeInfo = (soup.find_all(\"div\",  attrs={ \"class\" : \"spaceit_pad\"}))\n",
    "        #print(\"Animeinfo: \", animeInfo)\n",
    "        \n",
    "        animeType = \" \".join((findField(animeInfo, \"Type:\").a.text).split())\n",
    "        #print(\"animeType :\", animeType)\n",
    "        \n",
    "        animeNumEpisode = int(list(filter(lambda y: y != '',list(map(lambda x: \" \".join((x).split()),findField(animeInfo, \"Episodes:\").findAll(text=True, recursive=False)) )))[0])\n",
    "        #print(\"animeNumEpisode: \", animeNumEpisode)\n",
    "        \n",
    "        aired = list(filter(lambda y: y != '',list(map(lambda x: \" \".join((x).split()),findField(animeInfo, \"Aired:\").findAll(text=True, recursive=False)) )))[0].split('to')\n",
    "        releaseDate = datetime.datetime(int(aired[0].split()[2]), months.index(aired[0].split()[0])+1 , int(aired[0].split()[1][:1]))\n",
    "        if len(aired) == 2 and len(aired[0].split()) == 3 and len(aired[1].split()) == 3:\n",
    "            endDate = datetime.datetime(int(aired[1].split()[2]), months.index(aired[1].split()[0])+1 , int(aired[1].split()[1][:1]))\n",
    "        else:\n",
    "            endDate = \"\"\n",
    "        #print(\"releaseDate: \", releaseDate)\n",
    "        #print(\"endDate: \", endDate)\n",
    "        \n",
    "        animeNumMembers = int(list(filter(lambda y: y != '',list(map(lambda x: \" \".join((x).split()),findField(animeInfo, \"Members:\").findAll(text=True, recursive=False)) )))[0].replace(\",\", \"\"))\n",
    "        #print(\"animeNumMembers: \", animeNumMembers)\n",
    "        \n",
    "        animeScore = float(\" \".join((soup.find_all(\"span\", itemprop = \"ratingValue\")[0].text).split()))\n",
    "        #print(\"animeScore: \", animeScore)\n",
    "        \n",
    "        animeUsers = int(\" \".join((soup.find_all(\"span\", itemprop = \"ratingCount\")[0].text).split()))\n",
    "        #print(\"animeUsers: \", animeUsers)\n",
    "        \n",
    "        animeRank = int(list(filter(lambda x: x[0] == '#',findField(animeInfo, \"Ranked:\").text.split()))[0].replace('#',''))\n",
    "        #print(\"animeRank: \", animeRank)\n",
    "\n",
    "        animePopularity = int(list(filter(lambda y: y != '',list(map(lambda x: \" \".join((x).split()),findField(animeInfo, \"Popularity:\").findAll(text=True, recursive=False)) )))[0].replace(\"#\", \"\"))\n",
    "        #print(\"animePopularity: \", animePopularity)\n",
    "        \n",
    "        animeDescription = soup.find_all(\"p\", itemprop = \"description\")[0].text.strip().replace('\\n', '').replace('  ', '')\n",
    "        #print(\"animeDescription: \", animeDescription)\n",
    "        \n",
    "        animeRelatedAHref = soup.find(name=\"table\",attrs={\"class\":\"anime_detail_related_anime\"}).findChildren('a', href=True)\n",
    "        animeRelated = []\n",
    "        for x in animeRelatedAHref:\n",
    "            aux = \" \".join((x.text).split())\n",
    "            if aux not in animeRelated:\n",
    "                animeRelated.append(aux)\n",
    "        #print(\"animeRelated: \", animeRelated)\n",
    "        \n",
    "        animeCharactersRaw = soup.find_all(\"h3\", attrs={\"class\" : \"h3_characters_voice_actors\"})\n",
    "        animeCharacters = []\n",
    "        for x in animeCharactersRaw:\n",
    "            animeCharacters.append(\" \".join((x.text).split()))\n",
    "        #print(\"animeCharacters: \", animeCharacters)\n",
    "        \n",
    "        animeVoicesRaw = soup.find_all(\"td\", attrs={\"class\" : \"va-t ar pl4 pr4\"})\n",
    "        animeVoices = []\n",
    "        for x in animeVoicesRaw:\n",
    "            animeVoices.append(\" \".join((x.contents[1].text).split()))\n",
    "        #print(\"animeVoices: \", animeVoices)\n",
    "        \n",
    "        animeStaff = \"\"\n",
    "        if(len(soup.find_all(\"div\", attrs={\"class\" : \"detail-characters-list clearfix\"})) == 2):\n",
    "            aux = soup.find_all(\"div\", attrs={\"class\" : \"detail-characters-list clearfix\"})[1]\n",
    "            animeStaffRaw = []\n",
    "            for x in aux.findChildren('a'):\n",
    "                if (\" \".join((x.text).split()) != ''):\n",
    "                    animeStaffRaw.append(\" \".join((x.text).split()))\n",
    "            animeStaffTaskRaw = []\n",
    "            for x in aux.findChildren('small'):\n",
    "                animeStaffTaskRaw.append(\" \".join((x.text).split()))\n",
    "            #print(\"animeStaffRaw: \", animeStaffRaw)\n",
    "            #print(\"animeStaffTaskRaw: \", animeStaffTaskRaw)\n",
    "            animeStaff = [list(a) for a in zip(animeStaffRaw, animeStaffTaskRaw)]\n",
    "\n",
    "        \n",
    "        #print(\"animeStaff: \", animeStaff)\n",
    "        \n",
    "        return [animeTitle, animeType, animeNumEpisode, releaseDate, endDate, animeNumMembers, animeScore, animeUsers, animeRank, animePopularity, animeDescription, animeRelated, animeCharacters, animeVoices, animeStaff]\n",
    "    \n",
    "    \n",
    "    \n",
    "def extractData2(pagePath):\n",
    "    with open(pagePath, encoding=\"utf-8\") as fp:\n",
    "        soup = BeautifulSoup(fp, \"html.parser\")\n",
    "        \n",
    "        try:\n",
    "            animeTitle = \" \".join((soup.html.head.title.text.replace(\"- MyAnimeList.net\", '')).split())\n",
    "        except:\n",
    "            animeTitle = \" \"\n",
    "        #print(\"animeTitle: \", animeTitle)\n",
    "        \n",
    "        try:\n",
    "            animeInfo = (soup.find_all(\"div\",  attrs={ \"class\" : \"spaceit_pad\"}))\n",
    "        #print(\"Animeinfo: \", animeInfo)\n",
    "        except:\n",
    "            animeInfo = \" \"\n",
    "        \n",
    "        try:\n",
    "            animeType = \" \".join((findField(animeInfo, \"Type:\").a.text).split())\n",
    "        #print(\"animeType :\", animeType)\n",
    "        except:\n",
    "            animeType = \" \"\n",
    "        try:\n",
    "            animeNumEpisode = int(list(filter(lambda y: y != '',list(map(lambda x: \" \".join((x).split()),findField(animeInfo, \"Episodes:\").findAll(text=True, recursive=False)) )))[0])\n",
    "        #print(\"animeNumEpisode: \", animeNumEpisode)\n",
    "        except:\n",
    "            animeNumEpisode = \" \"\n",
    "        \n",
    "        try:\n",
    "            aired = list(filter(lambda y: y != '',list(map(lambda x: \" \".join((x).split()),findField(animeInfo, \"Aired:\").findAll(text=True, recursive=False)) )))[0].split('to')\n",
    "            releaseDate = datetime.datetime(int(aired[0].split()[2]), months.index(aired[0].split()[0])+1 , int(aired[0].split()[1][:1]))\n",
    "            if len(aired) == 2:\n",
    "                endDate = datetime.datetime(int(aired[1].split()[2]), months.index(aired[1].split()[0])+1 , int(aired[1].split()[1][:1]))\n",
    "            else:\n",
    "                endDate = \"\"\n",
    "        except:\n",
    "            releaseDate = \" \"\n",
    "            endDate = \" \"\n",
    "        #print(\"releaseDate: \", releaseDate)\n",
    "        #print(\"endDate: \", endDate)\n",
    "        \n",
    "        try:\n",
    "            animeNumMembers = int(list(filter(lambda y: y != '',list(map(lambda x: \" \".join((x).split()),findField(animeInfo, \"Members:\").findAll(text=True, recursive=False)) )))[0].replace(\",\", \"\"))\n",
    "        #print(\"animeNumMembers: \", animeNumMembers)\n",
    "        except:\n",
    "            animeNumMembers = \" \"\n",
    "        \n",
    "        try:\n",
    "            animeScore = float(\" \".join((soup.find_all(\"span\", itemprop = \"ratingValue\")[0].text).split()))\n",
    "        #print(\"animeScore: \", animeScore)\n",
    "        except:\n",
    "            animeScore = \" \"\n",
    "\n",
    "        try:\n",
    "            animeUsers = int(\" \".join((soup.find_all(\"span\", itemprop = \"ratingCount\")[0].text).split()))\n",
    "        #print(\"animeUsers: \", animeUsers)\n",
    "        except:\n",
    "            animeUsers = \" \"\n",
    "        \n",
    "        try:\n",
    "            animeRank = int(list(filter(lambda x: x[0] == '#',findField(animeInfo, \"Ranked:\").text.split()))[0].replace('#',''))\n",
    "        #print(\"animeRank: \", animeRank)\n",
    "        except:\n",
    "            animeRank = \" \"\n",
    "        \n",
    "        try:\n",
    "            animePopularity = int(list(filter(lambda y: y != '',list(map(lambda x: \" \".join((x).split()),findField(animeInfo, \"Popularity:\").findAll(text=True, recursive=False)) )))[0].replace(\"#\", \"\"))\n",
    "        #print(\"animePopularity: \", animePopularity)\n",
    "        except:\n",
    "            animePopularity = \" \"\n",
    "        \n",
    "        try:\n",
    "            animeDescription = soup.find_all(\"p\", itemprop = \"description\")[0].text.strip().replace('\\n', '').replace('  ', '')\n",
    "        #print(\"animeDescription: \", animeDescription)\n",
    "        except:\n",
    "            animeDescription = \" \"\n",
    "        \n",
    "        try:\n",
    "            animeRelatedAHref = soup.find(name=\"table\",attrs={\"class\":\"anime_detail_related_anime\"}).findChildren('a', href=True)\n",
    "            animeRelated = []\n",
    "            for x in animeRelatedAHref:\n",
    "                aux = \" \".join((x.text).split())\n",
    "            if aux not in animeRelated:\n",
    "                animeRelated.append(aux)\n",
    "        #print(\"animeRelated: \", animeRelated)\n",
    "        except:\n",
    "            animeRelated = \" \"\n",
    "        \n",
    "        try:\n",
    "            animeCharactersRaw = soup.find_all(\"h3\", attrs={\"class\" : \"h3_characters_voice_actors\"})\n",
    "            animeCharacters = []\n",
    "            for x in animeCharactersRaw:\n",
    "                animeCharacters.append(\" \".join((x.text).split()))\n",
    "        #print(\"animeCharacters: \", animeCharacters)\n",
    "        except:\n",
    "            animeCharacters = \" \"\n",
    "\n",
    "        try:\n",
    "            animeVoicesRaw = soup.find_all(\"td\", attrs={\"class\" : \"va-t ar pl4 pr4\"})\n",
    "            animeVoices = []\n",
    "            for x in animeVoicesRaw:\n",
    "                animeVoices.append(\" \".join((x.contents[1].text).split()))\n",
    "        #print(\"animeVoices: \", animeVoices)\n",
    "        except:\n",
    "            animeVoices = \" \"\n",
    "\n",
    "\n",
    "        try:\n",
    "            animeStaff = \"\"\n",
    "            if(len(soup.find_all(\"div\", attrs={\"class\" : \"detail-characters-list clearfix\"})) == 2):\n",
    "                aux = soup.find_all(\"div\", attrs={\"class\" : \"detail-characters-list clearfix\"})[1]\n",
    "                animeStaffRaw = []\n",
    "            for x in aux.findChildren('a'):\n",
    "                if (\" \".join((x.text).split()) != ''):\n",
    "                    animeStaffRaw.append(\" \".join((x.text).split()))\n",
    "            animeStaffTaskRaw = []\n",
    "            for x in aux.findChildren('small'):\n",
    "                animeStaffTaskRaw.append(\" \".join((x.text).split()))\n",
    "            #print(\"animeStaffRaw: \", animeStaffRaw)\n",
    "            #print(\"animeStaffTaskRaw: \", animeStaffTaskRaw)\n",
    "            animeStaff = [list(a) for a in zip(animeStaffRaw, animeStaffTaskRaw)]\n",
    "        except:\n",
    "            animeStaff = \" \"\n",
    "\n",
    "        \n",
    "        #print(\"animeStaff: \", animeStaff)\n",
    "        \n",
    "        return [animeTitle, animeType, animeNumEpisode, releaseDate, endDate, animeNumMembers, animeScore, animeUsers, animeRank, animePopularity, animeDescription, animeRelated, animeCharacters, animeVoices, animeStaff]\n",
    "\n",
    "def tsvGenerator():\n",
    "    c = 83\n",
    "    url_file = 'urls.txt'\n",
    "    num_lines = sum(1 for line in open(url_file))\n",
    "    pagesPerFolder = num_lines/nFolders\n",
    "    outputTSV = \"dataset.tsv\"\n",
    "\n",
    "    with open(outputTSV, 'wt') as out_file:\n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        tsv_writer.writerow([\"animeTitle\", \"animeType\", \"animeNumEpisode\", \"releaseDate\", \"endDate\", \"animeNumMembers\", \"animeScore\", \"animeUsers\", \"animeRank\", \"animePopularity\", \"animeDescription\", \"animeRelated\", \"animeCharacters\", \"animeVoices\", \"animeStaff\"])\n",
    "        for x in range(1, 20000 +1):\n",
    "\n",
    "            number = int(c/(pagesPerFolder)) + 1\n",
    "            path = \"./cartella{}\".format(number) + \"/page_{}.html\".format(c+1)\n",
    "            print(\"path: \", path)\n",
    "            tsv_writer.writerow(extractData(path))\n",
    "            c = c + 1\n",
    "\n",
    "def getUrl(lineNum, url_file):\n",
    "    with open(url_file) as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            if i == lineNum-1:\n",
    "                return line\n",
    "            elif i >= lineNum:\n",
    "                break\n",
    "\n",
    "def downloadOneFile(pageNum):\n",
    "    \n",
    "    num_lines = sum(1 for line in open(\"urls.txt\"))\n",
    "    pagesPerFolder = num_lines/nFolders\n",
    "    url = getUrl(pageNum, \"urls.txt\")\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, features =\"lxml\")\n",
    "    cartellaNumber = int(pageNum/(pagesPerFolder)) + 1\n",
    "    print(\"Cartella\" + str(cartellaNumber) + \".  Page_\"+ str(pageNum))\n",
    "    f = open(\"./cartella{}\".format(cartellaNumber) + \"/page_{}.html\".format(pageNum), \"w\",encoding=\"utf-8\")\n",
    "    f.write(soup.prettify())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZGsKI1ZXDsW"
   },
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hn-P5MHkXDsW"
   },
   "source": [
    "Since no dataset was provided for this homework, we had to create our own dataset by scraping the given website, which was done in the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AUXbQpFXDsW"
   },
   "source": [
    "## 1.1 Get the list of animes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cd64vchXDsX"
   },
   "source": [
    "Firstly, we had to get the list of all the anime we wanted to add to our dataset. This included all the anime found in the first 400 pages. Therefore, we created a function that scrapes a certain number of pages from an initial url, to extract the url of each of them. The initial url and the following number of pages are given as input, while the resulting urls are stored in a txt file called \"urls.txt\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLKIYSvwXDsX",
    "outputId": "08d91975-e683-4a21-8e1f-497072ce001e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [05:33<00:00,  1.20it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 950345.87it/s]\n"
     ]
    }
   ],
   "source": [
    "get_urls(\"https://myanimelist.net/topanime.php?limit \", 400)    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_L_dUibUXDsY"
   },
   "source": [
    "## 1.2 Crawl books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMf4nc-jXDsZ"
   },
   "outputs": [],
   "source": [
    "crawl(\"urls.txt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cyfwl5lXDsZ"
   },
   "outputs": [],
   "source": [
    "def get_urls(initial_url, n_pages): # get the url of each single book\n",
    "    UrlsFiles = open(\"urlpages.txt\", \"w\")\n",
    "\n",
    "    for i in range(1, n_pages+1):\n",
    "        url = initial_url +str(i)\n",
    "\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, features='lxml')\n",
    "        for a in soup.find_all('a', class_=\"bookTitle\"):\n",
    "            UrlsFiles.write(a.get('href')+'\\n')\n",
    "\n",
    "    UrlsFiles.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0JlkesJXDsZ",
    "outputId": "62f2c4e7-0018-4b0e-8779-37ba7e076ad4"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-30bad320dd9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://myanimelist.net/topanime.php?limit=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19950\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-1fa46780a499>\u001b[0m in \u001b[0;36mget_urls\u001b[0;34m(initial_url, n_pages)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_url\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bookTitle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    424\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1332\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_urls(\"https://myanimelist.net/topanime.php?limit=\", 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "2cNF8m7wecqs",
    "outputId": "e0891359-f30a-4f51-f760-95f42415064a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:  ./cartella2/page_84.html\n",
      "path:  ./cartella2/page_85.html\n",
      "path:  ./cartella2/page_86.html\n",
      "path:  ./cartella2/page_87.html\n",
      "path:  ./cartella2/page_88.html\n",
      "path:  ./cartella2/page_89.html\n",
      "path:  ./cartella2/page_90.html\n",
      "path:  ./cartella2/page_91.html\n",
      "path:  ./cartella2/page_92.html\n",
      "path:  ./cartella2/page_93.html\n",
      "path:  ./cartella2/page_94.html\n",
      "path:  ./cartella2/page_95.html\n",
      "path:  ./cartella2/page_96.html\n",
      "path:  ./cartella2/page_97.html\n",
      "path:  ./cartella2/page_98.html\n",
      "path:  ./cartella2/page_99.html\n",
      "path:  ./cartella2/page_100.html\n",
      "path:  ./cartella3/page_101.html\n",
      "path:  ./cartella3/page_102.html\n",
      "path:  ./cartella3/page_103.html\n",
      "path:  ./cartella3/page_104.html\n",
      "path:  ./cartella3/page_105.html\n",
      "path:  ./cartella3/page_106.html\n",
      "path:  ./cartella3/page_107.html\n",
      "path:  ./cartella3/page_108.html\n",
      "path:  ./cartella3/page_109.html\n",
      "path:  ./cartella3/page_110.html\n",
      "path:  ./cartella3/page_111.html\n",
      "path:  ./cartella3/page_112.html\n",
      "path:  ./cartella3/page_113.html\n",
      "path:  ./cartella3/page_114.html\n",
      "path:  ./cartella3/page_115.html\n",
      "path:  ./cartella3/page_116.html\n",
      "path:  ./cartella3/page_117.html\n",
      "path:  ./cartella3/page_118.html\n",
      "path:  ./cartella3/page_119.html\n",
      "path:  ./cartella3/page_120.html\n",
      "path:  ./cartella3/page_121.html\n",
      "path:  ./cartella3/page_122.html\n",
      "path:  ./cartella3/page_123.html\n",
      "path:  ./cartella3/page_124.html\n",
      "path:  ./cartella3/page_125.html\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7296/2200581846.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtsvGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7296/1503267733.py\u001b[0m in \u001b[0;36mtsvGenerator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./cartella{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/page_{}.html\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"path: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[0mtsv_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextractData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m             \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7296/1503267733.py\u001b[0m in \u001b[0;36mextractData\u001b[1;34m(pagePath)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;31m#print(\"Animeinfo: \", animeInfo)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0manimeType\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfindField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manimeInfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Type:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[1;31m#print(\"animeType :\", animeType)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'a'"
     ]
    }
   ],
   "source": [
    "tsvGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8QhD-G5zXDsa"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    stop_words = set(stopwords.words('english')) # obtain the stop words\n",
    "    good_words = [] # save the correct words to consider like tokens\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\") # function to recognize the tokens\n",
    "    words = tokenizer.tokenize(text) # tokenize the text \n",
    "    for word in words:\n",
    "        # check if the word is lower and it isn't a stop word or a number\n",
    "        if word.lower() not in stop_words and word.isalpha(): \n",
    "            word = PorterStemmer().stem(word) # use the stemmer function\n",
    "            good_words.append(word.lower()) # insert the good token to lower case\n",
    "        \n",
    "    return good_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmS56cL6XDsb"
   },
   "outputs": [],
   "source": [
    "a = \"I'm Michele, and I live in Ostia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWk_Ss-OXDsb",
    "outputId": "a613eb22-0eee-43a7-c790-5ecf4e500e3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['michel', 'live', 'ostia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_FOGyKyXDsb",
    "outputId": "6344ede2-d32a-473c-9677-d3b36ed5e976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework_3_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
