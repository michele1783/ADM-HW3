{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OAmHLj0XDsQ"
   },
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dE37ft_VXDsT"
   },
   "source": [
    "Letizia Russo, Daniel Losada Molina and Michele Luca Puzzo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiJCtCwWXDsa",
    "outputId": "d084f566-4d1e-4870-c12d-2860dc65ecc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/michele/opt/anaconda3/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: joblib in /Users/michele/opt/anaconda3/lib/python3.8/site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: click in /Users/michele/opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /Users/michele/opt/anaconda3/lib/python3.8/site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: tqdm in /Users/michele/opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.47.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "w5D60sG_XDsU"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml\n",
    "import shutil\n",
    "#import functions as fun\n",
    "import urllib.request\n",
    "import urllib.error as uer\n",
    "import time\n",
    "import nltk\n",
    "import datetime\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_list(anime):\n",
    "    with open('urls.txt', 'w') as file:\n",
    "        for page in range(0, anime+50, 50):\n",
    "            url = 'https://myanimelist.net/topanime.php?limit=' + str(page)\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content)\n",
    "            ranking_list = soup.find_all(class_='ranking-list')\n",
    "            for l in ranking_list:\n",
    "                link = l.find_all(class_='detail')[0].a['href']\n",
    "                file.write(\"%s\\n\" % link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jH1PMXgOZLmZ"
   },
   "outputs": [],
   "source": [
    "nFolders = 299\n",
    "\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun','Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "def crawl(url_file):\n",
    "    Path(\"directory\").mkdir(exist_ok=True)\n",
    "    for i in range(1,nFolders+1):\n",
    "        nomeCartella = 'cartelle/cartella{}'.format(i)\n",
    "        Path(nomeCartella).mkdir(exist_ok=True)\n",
    "    num_lines = sum(1 for line in open(url_file, encoding=\"utf-8\"))\n",
    "    pagesPerFolder = num_lines/nFolders\n",
    "    a = open(url_file, \"r\")\n",
    "    c = 384\n",
    "    lastFolder = 7\n",
    "    for i in a:\n",
    "        page = requests.get(i)\n",
    "        soup = BeautifulSoup(page.content, features =\"lxml\")\n",
    "        number = int(c/(pagesPerFolder)) + 1\n",
    "        if number != lastFolder:\n",
    "            print(\"I'm waiting\")\n",
    "            lastFolder = number\n",
    "            time.sleep(200)            \n",
    "        print(\"Going to save in cartella\" + str(number) + \", the page \" + str(c+1))\n",
    "        f = open(\"./cartelle/cartella{}\".format(number) + \"/page_{}.html\".format(c+1), \"w\",encoding=\"utf-8\")\n",
    "        f.write(soup.prettify())\n",
    "        f.close()\n",
    "        \n",
    "        file_size = os.path.getsize(\"./cartelle/cartella{}/page_{}.html\".format(number, c + 1))\n",
    "        if file_size < 50000:\n",
    "            os.remove(\"./cartelle/cartella{}/page_{}.html\".format(number, c + 1))\n",
    "            page = requests.get(i)\n",
    "            soup = BeautifulSoup(page.content, features =\"lxml\")\n",
    "            f = open(\"./cartelle/cartella{}\".format(number) + \"/page_{}.html\".format(c+1), \"w\",encoding=\"utf-8\")\n",
    "            f.write(soup.prettify())\n",
    "            f.close()\n",
    "                \n",
    "        else:\n",
    "            c = c + 1\n",
    "        \n",
    "def findField(array, word):\n",
    "    for x in array:\n",
    "        #print(\"#####################################\")\n",
    "        #print(\" \".join((x.find(\"span\").text).split()))\n",
    "        #print(\"#####################################\")\n",
    "        if \" \".join((x.find(\"span\").text).split()) == word:\n",
    "            return x\n",
    "    return -1       \n",
    "        \n",
    "        \n",
    "def cleaner(text):\n",
    "    stop_words = set(stopwords.words('english')) # retrieve stop words\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\") # recognize the tokens\n",
    "    parole = tokenizer.tokenize(text) # tokenize the text\n",
    "    correct_words = [] # save the correct words to consider like tokens\n",
    "    for word in parole:\n",
    "        # check if the word is lower and it isn't a stop word or a number\n",
    "        if word.lower() not in stop_words and word.isalpha(): \n",
    "            word = PorterStemmer().stem(word) # use the stemmer function\n",
    "            correct_words.append(word.lower()) # insert the good token to lower case\n",
    "        \n",
    "    return correct_words\n",
    "\n",
    "def create_vocabulary(data):\n",
    "    ### Input == I use like input the dataset obtain in exercise 1 where i apply the clean text function\n",
    "    ### Output == I obtain a vocabulary, the keys are all tokens (with no repeat) contained in the synopsis for the each rows\n",
    "    ### for each token I define the index of the rows where the token is in the synopsis\n",
    "    voc = {}\n",
    "    for i, row in data.iterrows():\n",
    "            if len(data.at[i, \"Synopsis\"]) > 0:  # check if the list is empty or not to avoid the eventually error\n",
    "                for word in data.at[i, \"Synopsis\"]: # bring the token from the list\n",
    "                    if word in voc.keys(): # insert the token into the vocabulary with the documents where this is present\n",
    "                        if i not in voc[word]:\n",
    "                            voc[word].append(i)\n",
    "                    else:\n",
    "                        voc[word] = [i]\n",
    "    return voc\n",
    "\n",
    "def extractData(pagePath):\n",
    "    with open(pagePath, encoding=\"utf-8\") as fp:\n",
    "        soup = BeautifulSoup(fp, \"html.parser\")\n",
    "        animeTitle = \" \".join((soup.html.head.title.text).split())\n",
    "        \n",
    "        #print(\"animeTitle: \", animeTitle)\n",
    "        animeInfo = (soup.find_all(\"div\",  attrs={ \"class\" : \"spaceit_pad\"}))\n",
    "        #print(\"Animeinfo: \", animeInfo)\n",
    "        \n",
    "        animeType = \" \".join((findField(animeInfo, \"Type:\").a.text).split())\n",
    "        #print(\"animeType :\", animeType)\n",
    "        \n",
    "        animeNumEpisode = int(list(filter(lambda y: y != '',list(map(lambda x: \" \".join((x).split()),findField(animeInfo, \"Episodes:\").findAll(text=True, recursive=False)) )))[0])\n",
    "        #print(\"animeNumEpisode: \", animeNumEpisode)\n",
    "        \n",
    "        aired = list(filter(lambda y: y != '',list(map(lambda x: \" \".join((x).split()),findField(animeInfo, \"Aired:\").findAll(text=True, recursive=False)) )))[0].split('to')\n",
    "        releaseDate = datetime.datetime(int(aired[0].split()[2]), months.index(aired[0].split()[0])+1 , int(aired[0].split()[1][:1]))\n",
    "        if len(aired) == 2 and len(aired[0].split()) == 3 and len(aired[1].split()) == 3:\n",
    "            endDate = datetime.datetime(int(aired[1].split()[2]), months.index(aired[1].split()[0])+1 , int(aired[1].split()[1][:1]))\n",
    "        else:\n",
    "            endDate = \"\"\n",
    "        #print(\"releaseDate: \", releaseDate)\n",
    "        #print(\"endDate: \", endDate)\n",
    "        \n",
    "        animeNumMembers = int(list(filter(lambda y: y != '',list(map(lambda x: \" \".join((x).split()),findField(animeInfo, \"Members:\").findAll(text=True, recursive=False)) )))[0].replace(\",\", \"\"))\n",
    "        #print(\"animeNumMembers: \", animeNumMembers)\n",
    "        \n",
    "        animeScore = float(\" \".join((soup.find_all(\"span\", itemprop = \"ratingValue\")[0].text).split()))\n",
    "        #print(\"animeScore: \", animeScore)\n",
    "        \n",
    "        animeUsers = int(\" \".join((soup.find_all(\"span\", itemprop = \"ratingCount\")[0].text).split()))\n",
    "        #print(\"animeUsers: \", animeUsers)\n",
    "        \n",
    "        animeRank = int(list(filter(lambda x: x[0] == '#',findField(animeInfo, \"Ranked:\").text.split()))[0].replace('#',''))\n",
    "        #print(\"animeRank: \", animeRank)\n",
    "\n",
    "        animePopularity = int(list(filter(lambda y: y != '',list(map(lambda x: \" \".join((x).split()),findField(animeInfo, \"Popularity:\").findAll(text=True, recursive=False)) )))[0].replace(\"#\", \"\"))\n",
    "        #print(\"animePopularity: \", animePopularity)\n",
    "        \n",
    "        animeDescription = soup.find_all(\"p\", itemprop = \"description\")[0].text.strip().replace('\\n', '').replace('  ', '')\n",
    "        #print(\"animeDescription: \", animeDescription)\n",
    "        \n",
    "        animeRelatedAHref = soup.find(name=\"table\",attrs={\"class\":\"anime_detail_related_anime\"}).findChildren('a', href=True)\n",
    "        animeRelated = []\n",
    "        for x in animeRelatedAHref:\n",
    "            aux = \" \".join((x.text).split())\n",
    "            if aux not in animeRelated:\n",
    "                animeRelated.append(aux)\n",
    "        #print(\"animeRelated: \", animeRelated)\n",
    "        \n",
    "        animeCharactersRaw = soup.find_all(\"h3\", attrs={\"class\" : \"h3_characters_voice_actors\"})\n",
    "        animeCharacters = []\n",
    "        for x in animeCharactersRaw:\n",
    "            animeCharacters.append(\" \".join((x.text).split()))\n",
    "        #print(\"animeCharacters: \", animeCharacters)\n",
    "        \n",
    "        animeVoicesRaw = soup.find_all(\"td\", attrs={\"class\" : \"va-t ar pl4 pr4\"})\n",
    "        animeVoices = []\n",
    "        for x in animeVoicesRaw:\n",
    "            animeVoices.append(\" \".join((x.contents[1].text).split()))\n",
    "        #print(\"animeVoices: \", animeVoices)\n",
    "        \n",
    "        animeStaff = \"\"\n",
    "        if(len(soup.find_all(\"div\", attrs={\"class\" : \"detail-characters-list clearfix\"})) == 2):\n",
    "            aux = soup.find_all(\"div\", attrs={\"class\" : \"detail-characters-list clearfix\"})[1]\n",
    "            animeStaffRaw = []\n",
    "            for x in aux.findChildren('a'):\n",
    "                if (\" \".join((x.text).split()) != ''):\n",
    "                    animeStaffRaw.append(\" \".join((x.text).split()))\n",
    "            animeStaffTaskRaw = []\n",
    "            for x in aux.findChildren('small'):\n",
    "                animeStaffTaskRaw.append(\" \".join((x.text).split()))\n",
    "            #print(\"animeStaffRaw: \", animeStaffRaw)\n",
    "            #print(\"animeStaffTaskRaw: \", animeStaffTaskRaw)\n",
    "            animeStaff = [list(a) for a in zip(animeStaffRaw, animeStaffTaskRaw)]\n",
    "\n",
    "        \n",
    "        #print(\"animeStaff: \", animeStaff)\n",
    "        \n",
    "        return [animeTitle, animeType, animeNumEpisode, releaseDate, endDate, animeNumMembers, animeScore, animeUsers, animeRank, animePopularity, animeDescription, animeRelated, animeCharacters, animeVoices, animeStaff]\n",
    "    \n",
    "    \n",
    "    \n",
    "def extractData2(pagePath):\n",
    "    with open(pagePath, encoding=\"utf-8\") as fp:\n",
    "        soup = BeautifulSoup(fp, \"html.parser\")\n",
    "        \n",
    "        try:\n",
    "            animeTitle = \" \".join((soup.html.head.title.text.replace(\"- MyAnimeList.net\", '')).split())\n",
    "        except:\n",
    "            animeTitle = \" \"\n",
    "        #print(\"animeTitle: \", animeTitle)\n",
    "        \n",
    "        try:\n",
    "            animeInfo = (soup.find_all(\"div\",  attrs={ \"class\" : \"spaceit_pad\"}))\n",
    "        #print(\"Animeinfo: \", animeInfo)\n",
    "        except:\n",
    "            animeInfo = \" \"\n",
    "        \n",
    "        try:\n",
    "            animeType = \" \".join((findField(animeInfo, \"Type:\").a.text).split())\n",
    "        #print(\"animeType :\", animeType)\n",
    "        except:\n",
    "            animeType = \" \"\n",
    "        try:\n",
    "            animeNumEpisode = int(list(filter(lambda y: y != '',list(map(lambda x: \" \".join((x).split()),findField(animeInfo, \"Episodes:\").findAll(text=True, recursive=False)) )))[0])\n",
    "        #print(\"animeNumEpisode: \", animeNumEpisode)\n",
    "        except:\n",
    "            animeNumEpisode = \" \"\n",
    "        \n",
    "        try:\n",
    "            aired = list(filter(lambda y: y != '',list(map(lambda x: \" \".join((x).split()),findField(animeInfo, \"Aired:\").findAll(text=True, recursive=False)) )))[0].split('to')\n",
    "            releaseDate = datetime.datetime(int(aired[0].split()[2]), months.index(aired[0].split()[0])+1 , int(aired[0].split()[1][:1]))\n",
    "            if len(aired) == 2:\n",
    "                endDate = datetime.datetime(int(aired[1].split()[2]), months.index(aired[1].split()[0])+1 , int(aired[1].split()[1][:1]))\n",
    "            else:\n",
    "                endDate = \"\"\n",
    "        except:\n",
    "            releaseDate = \" \"\n",
    "            endDate = \" \"\n",
    "        #print(\"releaseDate: \", releaseDate)\n",
    "        #print(\"endDate: \", endDate)\n",
    "        \n",
    "        try:\n",
    "            animeNumMembers = int(list(filter(lambda y: y != '',list(map(lambda x: \" \".join((x).split()),findField(animeInfo, \"Members:\").findAll(text=True, recursive=False)) )))[0].replace(\",\", \"\"))\n",
    "        #print(\"animeNumMembers: \", animeNumMembers)\n",
    "        except:\n",
    "            animeNumMembers = \" \"\n",
    "        \n",
    "        try:\n",
    "            animeScore = float(\" \".join((soup.find_all(\"span\", itemprop = \"ratingValue\")[0].text).split()))\n",
    "        #print(\"animeScore: \", animeScore)\n",
    "        except:\n",
    "            animeScore = \" \"\n",
    "\n",
    "        try:\n",
    "            animeUsers = int(\" \".join((soup.find_all(\"span\", itemprop = \"ratingCount\")[0].text).split()))\n",
    "        #print(\"animeUsers: \", animeUsers)\n",
    "        except:\n",
    "            animeUsers = \" \"\n",
    "        \n",
    "        try:\n",
    "            animeRank = int(list(filter(lambda x: x[0] == '#',findField(animeInfo, \"Ranked:\").text.split()))[0].replace('#',''))\n",
    "        #print(\"animeRank: \", animeRank)\n",
    "        except:\n",
    "            animeRank = \" \"\n",
    "        \n",
    "        try:\n",
    "            animePopularity = int(list(filter(lambda y: y != '',list(map(lambda x: \" \".join((x).split()),findField(animeInfo, \"Popularity:\").findAll(text=True, recursive=False)) )))[0].replace(\"#\", \"\"))\n",
    "        #print(\"animePopularity: \", animePopularity)\n",
    "        except:\n",
    "            animePopularity = \" \"\n",
    "        \n",
    "        try:\n",
    "            animeDescription = soup.find_all(\"p\", itemprop = \"description\")[0].text.strip().replace('\\n', '').replace('  ', '')\n",
    "        #print(\"animeDescription: \", animeDescription)\n",
    "        except:\n",
    "            animeDescription = \" \"\n",
    "        \n",
    "        try:\n",
    "            animeRelatedAHref = soup.find(name=\"table\",attrs={\"class\":\"anime_detail_related_anime\"}).findChildren('a', href=True)\n",
    "            animeRelated = []\n",
    "            for x in animeRelatedAHref:\n",
    "                aux = \" \".join((x.text).split())\n",
    "            if aux not in animeRelated:\n",
    "                animeRelated.append(aux)\n",
    "        #print(\"animeRelated: \", animeRelated)\n",
    "        except:\n",
    "            animeRelated = \" \"\n",
    "        \n",
    "        try:\n",
    "            animeCharactersRaw = soup.find_all(\"h3\", attrs={\"class\" : \"h3_characters_voice_actors\"})\n",
    "            animeCharacters = []\n",
    "            for x in animeCharactersRaw:\n",
    "                animeCharacters.append(\" \".join((x.text).split()))\n",
    "        #print(\"animeCharacters: \", animeCharacters)\n",
    "        except:\n",
    "            animeCharacters = \" \"\n",
    "\n",
    "        try:\n",
    "            animeVoicesRaw = soup.find_all(\"td\", attrs={\"class\" : \"va-t ar pl4 pr4\"})\n",
    "            animeVoices = []\n",
    "            for x in animeVoicesRaw:\n",
    "                animeVoices.append(\" \".join((x.contents[1].text).split()))\n",
    "        #print(\"animeVoices: \", animeVoices)\n",
    "        except:\n",
    "            animeVoices = \" \"\n",
    "\n",
    "\n",
    "        try:\n",
    "            animeStaff = \"\"\n",
    "            if(len(soup.find_all(\"div\", attrs={\"class\" : \"detail-characters-list clearfix\"})) == 2):\n",
    "                aux = soup.find_all(\"div\", attrs={\"class\" : \"detail-characters-list clearfix\"})[1]\n",
    "                animeStaffRaw = []\n",
    "            for x in aux.findChildren('a'):\n",
    "                if (\" \".join((x.text).split()) != ''):\n",
    "                    animeStaffRaw.append(\" \".join((x.text).split()))\n",
    "            animeStaffTaskRaw = []\n",
    "            for x in aux.findChildren('small'):\n",
    "                animeStaffTaskRaw.append(\" \".join((x.text).split()))\n",
    "            #print(\"animeStaffRaw: \", animeStaffRaw)\n",
    "            #print(\"animeStaffTaskRaw: \", animeStaffTaskRaw)\n",
    "            animeStaff = [list(a) for a in zip(animeStaffRaw, animeStaffTaskRaw)]\n",
    "        except:\n",
    "            animeStaff = \" \"\n",
    "\n",
    "        \n",
    "        #print(\"animeStaff: \", animeStaff)\n",
    "        \n",
    "        return [animeTitle, animeType, animeNumEpisode, releaseDate, endDate, animeNumMembers, animeScore, animeUsers, animeRank, animePopularity, animeDescription, animeRelated, animeCharacters, animeVoices, animeStaff]\n",
    "\n",
    "def tsvGenerator():\n",
    "    c = 83\n",
    "    url_file = 'urls.txt'\n",
    "    num_lines = sum(1 for line in open(url_file))\n",
    "    pagesPerFolder = num_lines/nFolders\n",
    "    outputTSV = \"dataset.tsv\"\n",
    "\n",
    "    with open(outputTSV, 'wt') as out_file:\n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        tsv_writer.writerow([\"animeTitle\", \"animeType\", \"animeNumEpisode\", \"releaseDate\", \"endDate\", \"animeNumMembers\", \"animeScore\", \"animeUsers\", \"animeRank\", \"animePopularity\", \"animeDescription\", \"animeRelated\", \"animeCharacters\", \"animeVoices\", \"animeStaff\"])\n",
    "        for x in range(1, 20000 +1):\n",
    "\n",
    "            number = int(c/(pagesPerFolder)) + 1\n",
    "            path = \"./cartella{}\".format(number) + \"/page_{}.html\".format(c+1)\n",
    "            print(\"path: \", path)\n",
    "            tsv_writer.writerow(extractData(path))\n",
    "            c = c + 1\n",
    "\n",
    "def getUrl(lineNum, url_file):\n",
    "    with open(url_file) as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            if i == lineNum-1:\n",
    "                return line\n",
    "            elif i >= lineNum:\n",
    "                break\n",
    "\n",
    "def downloadOneFile(pageNum):\n",
    "    \n",
    "    num_lines = sum(1 for line in open(\"urls.txt\"))\n",
    "    pagesPerFolder = num_lines/nFolders\n",
    "    url = getUrl(pageNum, \"urls.txt\")\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, features =\"lxml\")\n",
    "    cartellaNumber = int(pageNum/(pagesPerFolder)) + 1\n",
    "    print(\"Cartella\" + str(cartellaNumber) + \".  Page_\"+ str(pageNum))\n",
    "    f = open(\"./cartella{}\".format(cartellaNumber) + \"/page_{}.html\".format(pageNum), \"w\",encoding=\"utf-8\")\n",
    "    f.write(soup.prettify())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZGsKI1ZXDsW"
   },
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hn-P5MHkXDsW"
   },
   "source": [
    "Since no dataset was provided for this homework, we had to create our own dataset by scraping the given website, which was done in the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AUXbQpFXDsW"
   },
   "source": [
    "## 1.1 Get the list of animes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cd64vchXDsX"
   },
   "source": [
    "Firstly, we had to get the list of all the anime we wanted to add to our dataset. This included all the anime found in the first 400 pages. Therefore, we created a function that scrapes a certain number of pages from an initial url, to extract the url of each of them. The initial url and the following number of pages are given as input, while the resulting urls are stored in a txt file called \"urls.txt\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLKIYSvwXDsX",
    "outputId": "08d91975-e683-4a21-8e1f-497072ce001e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [05:33<00:00,  1.20it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 950345.87it/s]\n"
     ]
    }
   ],
   "source": [
    "link_list(20000)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_L_dUibUXDsY"
   },
   "source": [
    "## 1.2 Crawl books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMf4nc-jXDsZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to save in cartella7, the page 385\n",
      "Going to save in cartella7, the page 386\n",
      "Going to save in cartella7, the page 387\n",
      "Going to save in cartella7, the page 388\n",
      "Going to save in cartella7, the page 389\n",
      "Going to save in cartella7, the page 390\n",
      "Going to save in cartella7, the page 391\n",
      "Going to save in cartella7, the page 392\n",
      "Going to save in cartella7, the page 393\n",
      "Going to save in cartella7, the page 394\n",
      "Going to save in cartella7, the page 395\n",
      "Going to save in cartella7, the page 396\n",
      "Going to save in cartella7, the page 397\n",
      "Going to save in cartella7, the page 398\n",
      "Going to save in cartella7, the page 399\n",
      "Going to save in cartella7, the page 400\n",
      "Going to save in cartella7, the page 401\n",
      "Going to save in cartella7, the page 402\n",
      "Going to save in cartella7, the page 403\n",
      "Going to save in cartella7, the page 404\n",
      "Going to save in cartella7, the page 405\n",
      "Going to save in cartella7, the page 406\n",
      "Going to save in cartella7, the page 407\n",
      "Going to save in cartella7, the page 408\n",
      "Going to save in cartella7, the page 409\n",
      "Going to save in cartella7, the page 409\n",
      "Going to save in cartella7, the page 410\n",
      "Going to save in cartella7, the page 411\n",
      "Going to save in cartella7, the page 412\n",
      "Going to save in cartella7, the page 413\n",
      "Going to save in cartella7, the page 414\n",
      "Going to save in cartella7, the page 415\n",
      "Going to save in cartella7, the page 416\n",
      "Going to save in cartella7, the page 417\n",
      "Going to save in cartella7, the page 418\n",
      "Going to save in cartella7, the page 419\n",
      "Going to save in cartella7, the page 420\n",
      "Going to save in cartella7, the page 421\n",
      "Going to save in cartella7, the page 422\n",
      "Going to save in cartella7, the page 423\n",
      "Going to save in cartella7, the page 424\n",
      "Going to save in cartella7, the page 425\n",
      "Going to save in cartella7, the page 426\n",
      "Going to save in cartella7, the page 427\n",
      "Going to save in cartella7, the page 428\n",
      "Going to save in cartella7, the page 429\n",
      "Going to save in cartella7, the page 430\n",
      "Going to save in cartella7, the page 431\n",
      "Going to save in cartella7, the page 432\n",
      "Going to save in cartella7, the page 433\n",
      "Going to save in cartella7, the page 434\n",
      "Going to save in cartella7, the page 435\n",
      "Going to save in cartella7, the page 436\n",
      "Going to save in cartella7, the page 437\n",
      "Going to save in cartella7, the page 438\n",
      "Going to save in cartella7, the page 439\n",
      "Going to save in cartella7, the page 440\n",
      "Going to save in cartella7, the page 441\n",
      "Going to save in cartella7, the page 442\n",
      "Going to save in cartella7, the page 443\n",
      "Going to save in cartella7, the page 444\n",
      "Going to save in cartella7, the page 445\n",
      "Going to save in cartella7, the page 446\n",
      "Going to save in cartella7, the page 447\n",
      "Going to save in cartella7, the page 448\n",
      "I'm waiting\n",
      "Going to save in cartella8, the page 449\n",
      "Going to save in cartella8, the page 450\n",
      "Going to save in cartella8, the page 451\n",
      "Going to save in cartella8, the page 452\n",
      "Going to save in cartella8, the page 453\n",
      "Going to save in cartella8, the page 454\n",
      "Going to save in cartella8, the page 455\n",
      "Going to save in cartella8, the page 456\n",
      "Going to save in cartella8, the page 457\n",
      "Going to save in cartella8, the page 458\n",
      "Going to save in cartella8, the page 459\n",
      "Going to save in cartella8, the page 460\n",
      "Going to save in cartella8, the page 461\n",
      "Going to save in cartella8, the page 462\n",
      "Going to save in cartella8, the page 463\n",
      "Going to save in cartella8, the page 464\n",
      "Going to save in cartella8, the page 465\n",
      "Going to save in cartella8, the page 466\n",
      "Going to save in cartella8, the page 467\n",
      "Going to save in cartella8, the page 468\n",
      "Going to save in cartella8, the page 469\n",
      "Going to save in cartella8, the page 470\n",
      "Going to save in cartella8, the page 471\n",
      "Going to save in cartella8, the page 472\n",
      "Going to save in cartella8, the page 473\n",
      "Going to save in cartella8, the page 474\n",
      "Going to save in cartella8, the page 475\n",
      "Going to save in cartella8, the page 476\n",
      "Going to save in cartella8, the page 477\n",
      "Going to save in cartella8, the page 478\n",
      "Going to save in cartella8, the page 478\n",
      "Going to save in cartella8, the page 479\n",
      "Going to save in cartella8, the page 480\n",
      "Going to save in cartella8, the page 481\n",
      "Going to save in cartella8, the page 482\n",
      "Going to save in cartella8, the page 483\n",
      "Going to save in cartella8, the page 484\n",
      "Going to save in cartella8, the page 485\n",
      "Going to save in cartella8, the page 486\n",
      "Going to save in cartella8, the page 487\n",
      "Going to save in cartella8, the page 488\n",
      "Going to save in cartella8, the page 489\n",
      "Going to save in cartella8, the page 490\n",
      "Going to save in cartella8, the page 491\n",
      "Going to save in cartella8, the page 492\n",
      "Going to save in cartella8, the page 493\n",
      "Going to save in cartella8, the page 494\n",
      "Going to save in cartella8, the page 495\n",
      "Going to save in cartella8, the page 496\n",
      "Going to save in cartella8, the page 497\n",
      "Going to save in cartella8, the page 498\n",
      "Going to save in cartella8, the page 499\n",
      "Going to save in cartella8, the page 500\n",
      "Going to save in cartella8, the page 501\n",
      "Going to save in cartella8, the page 502\n",
      "Going to save in cartella8, the page 503\n",
      "Going to save in cartella8, the page 504\n",
      "Going to save in cartella8, the page 505\n",
      "Going to save in cartella8, the page 506\n",
      "Going to save in cartella8, the page 507\n",
      "Going to save in cartella8, the page 508\n",
      "Going to save in cartella8, the page 509\n",
      "Going to save in cartella8, the page 510\n",
      "Going to save in cartella8, the page 511\n",
      "Going to save in cartella8, the page 512\n",
      "I'm waiting\n"
     ]
    }
   ],
   "source": [
    "crawl(\"urls.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "2cNF8m7wecqs",
    "outputId": "e0891359-f30a-4f51-f760-95f42415064a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:  ./cartella2/page_84.html\n",
      "path:  ./cartella2/page_85.html\n",
      "path:  ./cartella2/page_86.html\n",
      "path:  ./cartella2/page_87.html\n",
      "path:  ./cartella2/page_88.html\n",
      "path:  ./cartella2/page_89.html\n",
      "path:  ./cartella2/page_90.html\n",
      "path:  ./cartella2/page_91.html\n",
      "path:  ./cartella2/page_92.html\n",
      "path:  ./cartella2/page_93.html\n",
      "path:  ./cartella2/page_94.html\n",
      "path:  ./cartella2/page_95.html\n",
      "path:  ./cartella2/page_96.html\n",
      "path:  ./cartella2/page_97.html\n",
      "path:  ./cartella2/page_98.html\n",
      "path:  ./cartella2/page_99.html\n",
      "path:  ./cartella2/page_100.html\n",
      "path:  ./cartella3/page_101.html\n",
      "path:  ./cartella3/page_102.html\n",
      "path:  ./cartella3/page_103.html\n",
      "path:  ./cartella3/page_104.html\n",
      "path:  ./cartella3/page_105.html\n",
      "path:  ./cartella3/page_106.html\n",
      "path:  ./cartella3/page_107.html\n",
      "path:  ./cartella3/page_108.html\n",
      "path:  ./cartella3/page_109.html\n",
      "path:  ./cartella3/page_110.html\n",
      "path:  ./cartella3/page_111.html\n",
      "path:  ./cartella3/page_112.html\n",
      "path:  ./cartella3/page_113.html\n",
      "path:  ./cartella3/page_114.html\n",
      "path:  ./cartella3/page_115.html\n",
      "path:  ./cartella3/page_116.html\n",
      "path:  ./cartella3/page_117.html\n",
      "path:  ./cartella3/page_118.html\n",
      "path:  ./cartella3/page_119.html\n",
      "path:  ./cartella3/page_120.html\n",
      "path:  ./cartella3/page_121.html\n",
      "path:  ./cartella3/page_122.html\n",
      "path:  ./cartella3/page_123.html\n",
      "path:  ./cartella3/page_124.html\n",
      "path:  ./cartella3/page_125.html\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7296/2200581846.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtsvGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7296/1503267733.py\u001b[0m in \u001b[0;36mtsvGenerator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./cartella{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/page_{}.html\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"path: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[0mtsv_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextractData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m             \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7296/1503267733.py\u001b[0m in \u001b[0;36mextractData\u001b[1;34m(pagePath)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;31m#print(\"Animeinfo: \", animeInfo)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0manimeType\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfindField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manimeInfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Type:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[1;31m#print(\"animeType :\", animeType)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'a'"
     ]
    }
   ],
   "source": [
    "tsvGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8QhD-G5zXDsa"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    stop_words = set(stopwords.words('english')) # obtain the stop words\n",
    "    good_words = [] # save the correct words to consider like tokens\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\") # function to recognize the tokens\n",
    "    words = tokenizer.tokenize(text) # tokenize the text \n",
    "    for word in words:\n",
    "        # check if the word is lower and it isn't a stop word or a number\n",
    "        if word.lower() not in stop_words and word.isalpha(): \n",
    "            word = PorterStemmer().stem(word) # use the stemmer function\n",
    "            good_words.append(word.lower()) # insert the good token to lower case\n",
    "        \n",
    "    return good_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmS56cL6XDsb"
   },
   "outputs": [],
   "source": [
    "a = \"I'm Michele, and I live in Ostia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWk_Ss-OXDsb",
    "outputId": "a613eb22-0eee-43a7-c790-5ecf4e500e3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['michel', 'live', 'ostia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_FOGyKyXDsb",
    "outputId": "6344ede2-d32a-473c-9677-d3b36ed5e976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework_3_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
