{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    stop_words = set(stopwords.words('english')) # obtain the stop words\n",
    "    good_words = [] # save the correct words to consider like tokens\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\") # function to recognize the tokens\n",
    "    words = tokenizer.tokenize(text) # tokenize the text \n",
    "    for word in words:\n",
    "        # check if the word is lower and it isn't a stop word or a number\n",
    "        if word.lower() not in stop_words and word.isalpha(): \n",
    "            word = PorterStemmer().stem(word) # use the stemmer function\n",
    "            good_words.append(word.lower()) # insert the good token to lower case\n",
    "        \n",
    "    return good_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(df):\n",
    "    ### Input == I use like input the dataset obtain in exercise 1 where i apply the clean text function\n",
    "    ### Output == I obtain a vocabulary, the keys are all tokens (with no repeat) contained in the plot for the each rows\n",
    "    ### for each token I define the index of the rows where the token is in the plot\n",
    "    vocabulary = {}\n",
    "    for i, row in df.iterrows():\n",
    "            if len(df.at[i, \"Plot\"]) > 0:  # check if the list is empty or not to avoid the eventually error\n",
    "                for word in df.at[i, \"Plot\"]: # bring the token from the list\n",
    "                    if word in vocabulary.keys(): # insert the token into the vocabulary with the documents where this is present\n",
    "                        if i not in vocabulary[word]:\n",
    "                            vocabulary[word].append(i)\n",
    "                    else:\n",
    "                        vocabulary[word] = [i]\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scraping(input_path):\n",
    "    path = str(\"./\"+input_path)\n",
    "\n",
    "    filenames = os.listdir(path)\n",
    "    for i in range(1, 301):\n",
    "        filenames = os.listdir(path + '/' + str(i))\n",
    "\n",
    "        for file in filenames:\n",
    "            with open(path + '/' + str(i) + './article_'+str(file.split(\"_\")[1].replace(\".html\", \"\"))+'.tsv', 'w', encoding=\"utf-8\", newline='') as out_file: # create for each html article its article_i.tsv according to the professor requests!\n",
    "                tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "                tsv_writer.writerow(['bookTitle', 'bookSeries', 'bookAuthors', 'ratingValue', \n",
    "                                'ratingCount', 'reviewCount', 'Plot', 'NumberofPages', 'Published',\n",
    "                                'Characters', 'Setting', 'Url'])\n",
    "                scrap_book(tsv_writer, path + '/' + str(i) + \"/\" + file)\n",
    "\n",
    "# creating the final dataset\n",
    "def finaldataset_tsv(initial_path):\n",
    "    path = str(\"./\"+initial_path)\n",
    "    suffix = \".tsv\" \n",
    "    filenames = os.listdir(path) #\n",
    "    data2 = pd.DataFrame()\n",
    "\n",
    "    for i in range(1, 301):\n",
    "        filenames = os.listdir(path + '/' + str(i))\n",
    "        for file in filenames:\n",
    "            if file.endswith(suffix): \n",
    "                with open(path + '/' + str(i) + '/article_'+str(file.split(\"_\")[1]), 'r', encoding=\"utf-8\", newline='') as out_file:\n",
    "                        df = pd.read_csv(out_file,sep = \"\\t\")\n",
    "                        if  df.loc[0,\"Plot\"] != \" \" and df.loc[0,\"bookTitle\"] != \" \":\n",
    "                            data2 = pd.concat([data2,df])\n",
    "                            \n",
    "    with open(\"finaloutput.tsv\", \"w\", encoding=\"utf-8\", newline=\"\") as text_file: text_file.write(data2.to_csv(index=False)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
